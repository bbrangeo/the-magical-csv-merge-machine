#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Feb  6 15:01:16 2017

@author: leo

TODO:
    - Safe file name / not unique per date
    
    - API: List of internal referentials
    - API: List of finished modules for given project / source
    - API: List of loaded sources
    
    - API: Fetch logs
    - API: Move implicit load out of API
    
    - API: Error codes / remove error
    
    - Use logging module
    
    - Change metadata to use_internal and ref_name to last used or smt. Data to
      use is specified on api call and not read from metadata (unless using last used)
    
    - Protect admin functions
    -
    - ADD LICENSE

    - Do not go to next page if an error occured
    - General error handling
    
    - transform_and_download button
    - Download config
    
    - DOWNLOAD full config
    
    - User account
    - Auto train 
    
    - ABSOLUTELY:  handle memory issues
    - Allocate memory by user/ by IP?
    
    - look why I can get 90% or 30% match on same file
    - Study impact of training set size on match rate
    - POSTGRES all this ish
    
    - Choose btw add/select/upload and read/load/get
    
    - Catch exceptions. Never redirect if server error
    
    - Re-Run inference if selecting more columns

    - Dealing with inference parameters when columns change...

    - integration test
    
    - Error if job failed
    
    - Avoid import in scheduled 
    - fix cancel job
    
    - test slack integration


DEV GUIDELINES:
    - By default the API will use the file with the same name in the last 
      module that was completed. Otherwise, you can specify the module to use file from
    - Suggestion methods shall be prefixed by infer (ex: infer_load_params, infer_mvs)
    - Suggestion methods shall can be plugged as input as params variable of transformation modules
    - Single file modules shall take as input: (pandas_dataframe, params)
    - Single file modules suggestion modules shall ouput (params, log)
    - Single file modules replacement modules shall ouput (pandas_dataframe, log)
    
    - Multiple file modules shall take as input: (pd_dataframe_1, pd_dataframe_2, params)
    - Multiple file modules suggestion modules shall ouput params, log
    - Multiple file modules merge module shall ouput ???
    
    - run_info should contain fields: has_modifications and modified_columns
    
    - Files generated by modules should be in module directory and have names determined at the project level (not API, nor module)
    
    - Do NOT return files, instead write files which users can fetch file through the API
    - If bad params are passed to modules, exceptions are raised, it is the 
        APIs role to transform these exceptions in messages
    - Functions to check parameters should be named _check_{variable_or_function} (ex: _check_file_role)
    - All securing will be done in the API part
    - Always return {"error": ..., "project_id": ..., "response": ...} ???

    - All methods to load specific configs should raise an error if the config is not coherent
    - For each module, store user input
    
    - Load all configurations to project variables
    
    - Use _init_project when project_type is a variable in path
    
    - Always include project_type as variable or hardcode
    - Put module name before project_type if it exists for all project_type
    - Put module name after project_type if it exists only for this project_type (only with linker)
    - Put in API code modules that are of use only for the API

NOTES:
    - Pay for persistant storage?

# Download metadata
curl -i http://127.0.0.1:5000/metadata/ -X POST -F "request_json=@sample_download_request.json;type=application/json"

USES: /python-memcached

"""
import json
import os

# Change current path to path of api.py
curdir = os.path.dirname(os.path.realpath(__file__))
os.chdir(curdir)

# Flask imports
import flask
from flask import Flask, jsonify, render_template, request, send_file, url_for
from flask_session import Session
from flask_socketio import disconnect, emit, SocketIO
from flask_cors import CORS, cross_origin
from werkzeug.utils import secure_filename

# Redis imports
from rq import cancel_job as rq_cancel_job, Queue
from rq.job import Job
from worker import conn

import api_queued_modules

from admin import Admin
from normalizer import UserNormalizer
from linker import UserLinker

#==============================================================================
# INITIATE APPLICATION
#==============================================================================

# Initiate application
app = Flask(__name__)
cors = CORS(app)    
app.config['CORS_HEADERS'] = 'Content-Type'
#app.config['SERVER_NAME'] = '127.0.0.1:5000'
app.config['SESSION_TYPE'] = "memcached"# 'memcached'

Session(app)

app.debug = True
app.config['SECRET_KEY'] = open('secret_key.txt').read()
app.config['MAX_CONTENT_LENGTH'] = 2 * 1024 * 1024 * 1024 # Check that files are not too big (2GB)

socketio = SocketIO(app)       

# Redis connection
q = Queue(connection=conn, default_timeout=1800)

#==============================================================================
# HELPER FUNCTIONS
#==============================================================================
    
def _check_privilege(privilege):
    if privilege not in ['user', 'admin']:
        raise Exception('privilege can be only user or admin')

def _check_project_type(project_type):
    if project_type not in ['normalize', 'link']:
        raise Exception('project type can be only normalize or link')

def _check_file_role(file_role):
    if file_role not in ['ref', 'source']:
        raise Exception('File type should be ref or source')

def _check_request():
    '''Check that input request is valid'''
    pass

def _parse_request():
    '''
    Separates data information from parameters and assures that values in data
    parameters are safe
    '''
    # Parse json request
    data_params = None
    module_params = None
    if request.json:
        req = request.json
        assert isinstance(req, dict)
    
        if 'data_params' in req:
            data_params = req['data_params']
            
            # Make paths secure
            for key, value in data_params.items():
                data_params[key] = secure_filename(value)
            
        if 'module_params' in req:
            module_params = req['module_params']
    
    return data_params, module_params
    
def _parse_linking_request():
    data_params = None
    module_params = None
    if request.json:
        params = request.json
        assert isinstance(params, dict)
    
        if 'data_params' in params:
            data_params = params['data_params']
            for file_role in ['ref', 'source']:
                # Make paths secure
                for key, value in data_params[file_role].items():
                    data_params[file_role][key] = secure_filename(value)
                
        if 'module_params' in params:
            module_params = params['module_params']
    
    return data_params, module_params    


def _init_project(project_type, 
                 project_id=None, 
                 create_new=False, 
                 display_name=None, 
                 description=None):
    '''
    Runs the appropriate constructor for Linker or Normalizer projects
    
    DEV NOTE: Use this in api calls that have project_type as a variable
    '''
    _check_project_type(project_type)
    
    if project_type == 'link':
        proj = UserLinker(project_id=project_id, 
                          create_new=create_new, 
                          display_name=display_name, 
                          description=description)
    else:
        proj = UserNormalizer(project_id=project_id, 
                              create_new=create_new, 
                              display_name=display_name, 
                              description=description)
    return proj
            


#==============================================================================
# Error handling
#==============================================================================


@app.errorhandler(404)
def page_not_found(error):
    app.logger.error('URL not valid: %s', (error))
    return jsonify(error=True, message=error.description), 404

@app.errorhandler(405)
def method_not_allowed(error):
    app.logger.error('Method not allowed (POST or GET): %s', (error))
    return jsonify(error=True, message=error.description), 404

#@app.errorhandler(Exception)
#def internal_server_error(error):
#    app.logger.error('Server Error: %s', (error))
#    return jsonify(error=True, message=error.__str__()), 500


#==============================================================================
# WEB
#==============================================================================

@app.route('/')
@app.route('/web/', methods=['GET'])
@cross_origin()
def web_index():
    #  /!\ Partial URL. Full URL will depend on user form
    next_url_link = url_for('web_select_link_project') 
    next_url_normalize = url_for('web_normalize_select_file')
    
    return render_template('index.html',
                       next_url_link=next_url_link, 
                       next_url_normalize=next_url_normalize)

@app.route('/web/link/select_project/', methods=['GET'])
@cross_origin()
def web_select_link_project():
    next_url_partial = url_for('web_link_select_files', project_id='')
    new_link_project_api_url = url_for('new_project', project_type='link')
    delete_project_api_url_partial=url_for('delete_project', project_type='link', project_id='')
    exists_url_partial=url_for('project_exists', project_type='link', project_id='')
    
    admin = Admin()
    list_of_projects = admin.list_project_ids('link')
    
    return render_template('select_link_project.html',
                           list_of_projects = list_of_projects,
                           
                           next_url_partial=next_url_partial,
                           new_link_project_api_url=new_link_project_api_url, 
                           delete_project_api_url_partial=delete_project_api_url_partial,
                           exists_url_partial=exists_url_partial)
    
    
@app.route('/web/normalize/select_file/', methods=['GET']) # (Actually select_project)
@cross_origin()
def web_normalize_select_file():
    MAX_FILE_SIZE = 1048576
    
    next_url_partial = '/web/select_columns/normalize/' #url_for('web_mvs_normalize', file_name='') # Missing project_id and file_name    
    
    new_normalize_project_api_url = url_for('new_project', project_type='normalize')
    delete_normalize_project_api_url_partial=url_for('delete_project', 
                                                     project_type='normalize', 
                                                     project_id='')
    
    admin = Admin()
    all_user_projects = admin.list_projects('normalize') 
    return render_template('select_file_normalize.html', 
                           all_user_projects=all_user_projects,
                           new_normalize_project_api_url=new_normalize_project_api_url,
                           delete_normalize_project_api_url_partial = delete_normalize_project_api_url_partial,
                           
                           upload_api_url_partial=url_for('upload', project_id=''),
                           next_url_partial=next_url_partial,
                           MAX_FILE_SIZE=MAX_FILE_SIZE)    
    

# TODO: look into use of sessions for url generation
@app.route('/web/link/select_files/<project_id>', methods=['GET']) # (Actually select_projects)
@cross_origin()
def web_link_select_files(project_id):
    '''View to create or join 1 or 2 normalization projects (1 for norm, 2 for link)'''
    
    # TODO: Cannot select twice the same file
    MAX_FILE_SIZE = 1048576
    
    next_url = url_for('web_match_columns', project_id=project_id, file_role='source')
    
    new_normalize_project_api_url = url_for('new_project', project_type='normalize')
    delete_normalize_project_api_url_partial=url_for('delete_project', 
                                                     project_type='normalize', 
                                                     project_id='')

    # Do what you fina do
    admin = Admin()
    all_user_projects = admin.list_projects('normalize') # TODO: take care of this
    all_internal_projects = admin.list_projects('normalize') 

    # TODO: If you have link, also add files to current     
    return render_template('select_files_linker.html', 
                           project_id=project_id,
                           all_user_projects=all_user_projects, 
                           all_internal_projects=all_internal_projects,
                           new_normalize_project_api_url=new_normalize_project_api_url,
                           delete_normalize_project_api_url_partial = delete_normalize_project_api_url_partial,
                           
                           upload_api_url_partial=url_for('upload', project_id=''),
                           select_file_api_url=url_for('select_file', project_id=project_id),
                           next_url=next_url,
                           MAX_FILE_SIZE=MAX_FILE_SIZE)

@app.route('/web/select_columns/normalize/<project_id>/', methods=['GET']) # (Actually select_project)
@app.route('/web/select_columns/normalize/<project_id>/<file_name>/', methods=['GET']) # (Actually select_project)
@cross_origin()
def web_normalize_select_columns(project_id, file_name=None):
    '''
    Configurate file to return for ref and source in same page as well as columns
    that are supposed to match to test results (ex: Siren, SIRENE)
    '''
    # TODO: default to matching columns


    if file_name is None:
        proj = UserNormalizer(project_id)
        (_, file_name) = proj.get_last_written()

    ROWS_TO_DISPLAY = range(3)

    proj = UserNormalizer(project_id)

    proj.load_data('INIT', file_name, restrict_to_selected=False)
    samples = proj.get_sample(None, None, {'sample_ilocs':ROWS_TO_DISPLAY})
    
    selected_columns = proj.read_selected_columns()
    
    index = list(samples[0].keys())
    
    select_columns_normalize_api_url = url_for('add_selected_columns', 
                                          project_id=project_id)
    
    next_url = url_for('web_mvs_normalize', project_id=project_id, file_name=file_name)
    return render_template('select_columns_normalize.html', 
                           index=index,                           
                           samples=samples,
                           selected_columns=selected_columns,
                           select_columns_normalize_api_url=select_columns_normalize_api_url,
                           next_url=next_url)
    


# Three methods for type inference (i.e. try to guess the likeliest type for each CSV column)

@app.route('/web/infer_types/normalize/<project_id>/', methods=['GET'])
@app.route('/web/infer_types/normalize/<project_id>/<file_name>/', methods=['GET'])
@cross_origin()
def web_infertypes_normalize(project_id, file_name=None):
    # TODO: remove hack with file_name=None
    if file_name is None:
        proj = UserNormalizer(project_id)
        (_, file_name) = proj.get_last_written()
    
    next_url = url_for('web_launch_normalize', 
                       project_id=project_id, 
                       file_name=file_name)   
    return _web_infertypes_normalize(project_id, file_name, next_url)
    
@app.route('/web/infer_types/link/<project_id>/<file_role>/', methods=['GET'])
@cross_origin()   
def web_infertypes_link(project_id, file_role):
    _check_file_role(file_role)
    
    proj = UserLinker(project_id)
    normalize_project_id = proj.metadata['current'][file_role]['project_id']
    normalize_file_name = proj.metadata['current'][file_role]['file_name']
    
    if file_role == 'source':
        next_url = url_for('_web_infertypes_link', project_id=project_id, file_role='ref')
    else:
        next_url = url_for('web_mvs', project_id=project_id)
    return _web_infertypes_normalize(normalize_project_id, normalize_file_name, next_url)


def _web_infertypes_normalize(project_id, file_name, next_url):
    NUM_ROWS_TO_DISPLAY = 30
    # Show the same number of cell values (per column) as for missing values
    NUM_PER_COLUMN_TO_DISPLAY = 4          
    
    # Act on last file 
    proj = UserNormalizer(project_id)
    (module_name, file_name) = proj.get_last_written(None, file_name, before_module='normalize_types') 
    
    # Read config or perform inference for project
    proj.load_data(module_name, file_name)
    types_config = proj.read_config_data('normalizeTypes', 'config.json')
    if not types_config:
      # Infer data types and save
      inferredTypes = proj.infer('inferTypes', params = None)
    
    # Generate sample to display
    sample_params = {
                    'num_rows_to_display': NUM_ROWS_TO_DISPLAY,
                    'num_per_column_to_display': NUM_PER_COLUMN_TO_DISPLAY,
                    'drop_duplicates': True
                     }
    sample = proj.get_sample('sample_types', types_config, sample_params)

    formatted_inferred_types = dict()
    # An (input field name, likeliest type) dictionary
    formatted_inferred_types['columns'] = inferredTypes['dataTypes']
    # The list of all available types so the user can override any automatically inferred type
    formatted_inferred_types['all'] = preprocess_fields_v3.allDatatypes()
    
    data_params = {'module_name': module_name, 'file_name': file_name}

    return render_template('infer_types.html',
                           project_id = project_id, 
                           formatted_inferred_types = formatted_inferred_types,
                           index=list(sample[0].keys()),
                           sample=sample,                           
                           data_params=data_params,
                           add_config_api_url=url_for('upload_config', 
                                                      project_type='normalize',
                                                      project_id=project_id),
                           recode_missing_values_api_url=url_for('normalizeTypes', 
                                                      project_id=project_id),
                           next_url=next_url)


@app.route('/web/missing_values/normalize/<project_id>/', methods=['GET'])
@app.route('/web/missing_values/normalize/<project_id>/<file_name>/', methods=['GET'])
@cross_origin()
def web_mvs_normalize(project_id, file_name=None):
    # TODO: remove hack with file_name=None
    if file_name is None:
        proj = UserNormalizer(project_id)
        (_, file_name) = proj.get_last_written()
    
    next_url = url_for('web_launch_normalize', 
                       project_id=project_id, 
                       file_name=file_name)   
    return _web_mvs_normalize(project_id, file_name, next_url)
    
@app.route('/web/missing_values/link/<project_id>/<file_role>/', methods=['GET'])
@cross_origin()   
def web_mvs_link(project_id, file_role):
    _check_file_role(file_role)
    
    proj = UserLinker(project_id)
    normalize_project_id = proj.metadata['current'][file_role]['project_id']
    normalize_file_name = proj.metadata['current'][file_role]['file_name']
    
    if file_role == 'source':
        next_url = url_for('web_mvs_link', project_id=project_id, file_role='ref')
    else:
        next_url = url_for('web_dedupe', project_id=project_id)
    return _web_mvs_normalize(normalize_project_id, 
                              normalize_file_name, 
                              next_url)

def _web_mvs_normalize(project_id, file_name, next_url):
    NUM_ROWS_TO_DISPLAY = 30
    NUM_PER_MISSING_VAL_TO_DISPLAY = 4          
    # TODO: add click directly on cells with missing values
    
    # Act on last file 
    proj = UserNormalizer(project_id)
    (module_name, file_name) = proj.get_last_written(None, file_name, before_module='replace_mvs') 
    
    # Read config or perform inference for project
    proj.load_data(module_name, file_name)
    mvs_config = proj.read_config_data('replace_mvs', 'config.json')
    if not mvs_config:
        # Infer missing values + save
        mvs_config = proj.infer('infer_mvs', params=None)
    
    # Generate sample to display
    sample_params = {
                    'num_rows_to_display': NUM_ROWS_TO_DISPLAY,
                    'num_per_missing_val_to_display': NUM_PER_MISSING_VAL_TO_DISPLAY,
                    'drop_duplicates': True
                     }
    sample = proj.get_sample('sample_mvs', mvs_config, sample_params)
    
    # Format infered_mvs for display in web app
    formated_infered_mvs = dict()
    formated_infered_mvs['columns'] = {col:[mv['val'] for mv in mvs if mv['score'] >= mvs_config['thresh']] \
                    for col, mvs in mvs_config['mvs_dict']['columns'].items()}
    formated_infered_mvs['all'] = [mv['val'] for mv in mvs_config['mvs_dict']['all'] if mv['score'] >= mvs_config['thresh']]
    
    data_params = {'module_name': module_name, 'file_name': file_name}

    return render_template('missing_values.html',
                           project_id=project_id, 
                           formated_infered_mvs=formated_infered_mvs,
                           index=list(sample[0].keys()),
                           sample=sample,
                           
                           data_params=data_params,
                           add_config_api_url=url_for('upload_config', 
                                                      project_type='normalize',
                                                      project_id=project_id),
                           recode_missing_values_api_url=url_for('schedule_job',
                                                      job_name='replace_mvs',
                                                      project_id=project_id),
                           next_url=next_url)


@app.route('/web/link/match_columns/<project_id>/', methods=['GET'])
@cross_origin()
def web_match_columns(project_id):
    ROWS_TO_DISPLAY = range(3)
    
    proj = UserLinker(project_id)
    
    # 
    sample_params = {'sample_ilocs':ROWS_TO_DISPLAY}
    
    # Load source and regsample
    samples = dict()
    for file_role in ['ref', 'source']:
        proj.load_project_to_merge(file_role)
        (_, file_name) = proj.__dict__[file_role].get_last_written(module_name=None, 
                                                      file_name=None)
        proj.__dict__[file_role].load_data('INIT', 
                                         file_name, 
                                         nrows=max(ROWS_TO_DISPLAY)+1,
                                         restrict_to_selected=False)
        samples[file_role] = proj.__dict__[file_role].get_sample(None, None, sample_params)
        proj.__dict__[file_role].clear_memory()

    
    # Check valid onfirm valid columns for 
    def config_is_coherent(config, source_sample, ref_sample):
        do_break = False
        for pair in config:
            for col in pair['source']:
                if col not in source_sample[0]:
                    do_break = True
                    break
            for col in pair['ref']:
                if col not in ref_sample[0]:
                    do_break = True
                    break     
            if do_break:
                config = []
                break
        
        return config != []

    # Load previous config
    config = proj.read_col_matches()
    config = config * config_is_coherent(config, samples['source'], samples['ref'])                    
    
    return render_template('match_columns.html',
                           config=config,
                           
                           source_index=list(samples['source'][0].keys()),
                           ref_index=list(samples['ref'][0].keys()),
                           
                           source_sample=samples['source'],
                           ref_sample=samples['ref'],
                                                      
                           add_column_matches_api_url=url_for('add_column_matches', project_id=project_id),
                           next_url=url_for('web_mvs_link', project_id=project_id, file_role='source'))

    
@socketio.on('answer', namespace='/')
def web_get_answer(message_received):
    # TODO: avoid multiple click (front)
    # TODO: add safeguards  if not enough train (front)

    message_received = json.loads(message_received)
    print(message_received)
    project_id = message_received['project_id']
    user_input = message_received['user_input']
    
    
    message_to_display = ''
    #message = 'Expect to have about 50% of good proposals in this phase. The more you label, the better...'
    if flask._app_ctx_stack.labeller_mem[project_id]['labeller'].answer_is_valid(user_input):
        flask._app_ctx_stack.labeller_mem[project_id]['labeller'].parse_valid_answer(user_input)
        if flask._app_ctx_stack.labeller_mem[project_id]['labeller'].finished:
            print('Writing train')
            flask._app_ctx_stack.labeller_mem[project_id]['labeller'].write_training(flask._app_ctx_stack.labeller_mem[project_id]['paths']['train'])
            print('Wrote train')

            # TODO: Do dedupe
            next_url = url_for('web_select_return', project_type='link', project_id=project_id)
            emit('redirect', {'url': next_url})
        else:
            flask._app_ctx_stack.labeller_mem[project_id]['labeller'].new_label()
    else:
        message_to_display = 'Sent an invalid answer'
    emit('message', flask._app_ctx_stack.labeller_mem[project_id]['labeller'].to_emit(message=message_to_display))
    

@socketio.on('terminate', namespace='/')
def web_terminate_labeller_load(message_received):
    '''Clear memory in application for selected project'''
    message_received = json.loads(message_received)
    project_id = message_received['project_id']
    
    try:
        del flask._app_ctx_stack.labeller_mem[project_id]['labeller']
    except:
        print('Could not delete labeller')
    try:
        del flask._app_ctx_stack.labeller_mem[project_id]['paths']
    except:
        print('Could not delete paths')
            
    
@socketio.on('load_labeller', namespace='/')
def load_labeller(message_received):
    '''Loads labeller. Necessary to have a separate call to preload page'''    
    message_received = json.loads(message_received)
    project_id = message_received['project_id']
    
    # TODO: put variables in memory
    # TODO: remove from memory at the end
    proj = UserLinker(project_id=project_id)
    paths = proj._gen_paths_dedupe() 
    
    # Create flask labeller memory if necessary and add current labeller
    try:
        flask._app_ctx_stack.labeller_mem[project_id] = dict()
    except:
        flask._app_ctx_stack.labeller_mem = {project_id: dict()}
    
    # Generate dedupe paths and create labeller
    flask._app_ctx_stack.labeller_mem[project_id]['paths'] = paths
    flask._app_ctx_stack.labeller_mem[project_id]['labeller'] = proj._read_labeller()
    
    flask._app_ctx_stack.labeller_mem[project_id]['labeller'].new_label()
    emit('message', flask._app_ctx_stack.labeller_mem[project_id]['labeller'].to_emit(message=''))


@app.route('/web/link/dedupe_linker/<project_id>/', methods=['GET'])
@cross_origin()    
def web_dedupe(project_id):
    '''Labelling / training and matching using dedupe'''
    proj = UserLinker(project_id)
    
    source_id = proj.metadata['current']['source']['project_id']
    ref_id = proj.metadata['current']['source']['project_id']
    
    source_cat_api_url = url_for('schedule_job',
                                    job_name='concat_with_init', 
                                    project_id=source_id)
    ref_cat_api_url = url_for('schedule_job',
                                    job_name='concat_with_init', 
                                    project_id=ref_id)
    dummy_labeller = proj._gen_dedupe_dummy_labeller()
    
    return render_template('dedupe_training.html',
                           **dummy_labeller.to_emit(''),
                           create_labeller_api_url=url_for('schedule_job', job_name='create_labeller', project_id=project_id),
                           source_cat_api_url=source_cat_api_url,
                           ref_cat_api_url=ref_cat_api_url,
                           project_id=project_id)


@app.route('/web/select_return/<project_type>/<project_id>', methods=['GET'])
@cross_origin()
def web_select_return(project_type, project_id):
    '''
    Configurate file to return for ref and source in same page as well as columns
    that are supposed to match to test results (ex: Siren, SIRENE)
    '''
    # TODO: default to matching columns
    _check_project_type(project_type)
    if project_type == 'normalize':
        raise Exception('Normalize project_type is not supported for select_return')
    
    
    ROWS_TO_DISPLAY = range(3)
    
    proj = UserLinker(project_id)
    
    samples = dict()
    selected_columns_to_return = dict()

    for file_role in ['source', 'ref']:
        # Load sample
        data = proj.metadata['current'][file_role]
        
        proj.load_project_to_merge(file_role)       
        (module_name, file_name) = proj.__dict__[file_role].get_last_written(None, 
                                                    data['file_name'])

        # TODO: temporary solution here
        from api_queued_modules import _concat_with_init
        _concat_with_init(proj.__dict__[file_role].project_id, {'module_name': module_name, 'file_name': file_name})        
        # Reload after modifications
        proj.load_project_to_merge(file_role) 
    
        # TODO: figure out why columns are restricted
      
        proj.__dict__[file_role].load_data(
                        'concat_with_init', 
                        file_name,
                        nrows=max(ROWS_TO_DISPLAY)+1, 
                        restrict_to_selected=False)
        samples[file_role] = proj.__dict__[file_role].get_sample(None, None, {'sample_ilocs':ROWS_TO_DISPLAY})
        
        selected_columns_to_return[file_role] = proj.read_cols_to_return(file_role) 
    
    column_matches = proj.read_col_certain_matches()
    
    indexes = dict()
    indexes['source'] = list(samples['source'][0].keys())
    indexes['ref'] = list(samples['ref'][0].keys())
    
    select_return_api_urls = dict()
    select_return_api_urls['source'] = url_for('add_columns_to_return', 
                                          project_id=project_id, file_role='source')
    select_return_api_urls['ref'] = url_for('add_columns_to_return', 
                                          project_id=project_id, file_role='ref')
    
    next_url = url_for('web_download', project_type=project_type, project_id=project_id)
    return render_template('select_return.html', 
                           indexes=indexes,                           
                           samples=samples,
                           selected_columns_to_return=selected_columns_to_return,    
                           column_matches=column_matches,
                                                              
                           add_column_certain_matches_api_url=url_for(\
                                        'add_column_certain_matches', project_id=project_id),                          

                           select_return_api_urls=select_return_api_urls,
                           next_url=next_url)


@app.route('/web/download/normalize/<project_id>/<file_name>', methods=['GET'])
def web_launch_normalize(project_id, file_name):
    
    return render_template('last_page_normalize.html',
                           concat_with_init_api_url=url_for('schedule_job', job_name='concat_with_init', project_id=project_id),
                           home_url=url_for('web_index'))


@app.route('/web/download/<project_type>/<project_id>/', methods=['GET'])
def web_download(project_type, project_id):
    if project_type == 'normalize':
        raise NotImplementedError
    
    proj = _init_project(project_type, project_id=project_id)
    res_file_name = 'm3_result.csv'
    file_path = proj.path_to('dedupe_linker', res_file_name)    
    has_results =  os.path.isfile(file_path) 
    
    next_url = url_for('web_view_results', project_type='link', project_id=project_id)
    return render_template('last_page_link.html', 
                           has_results=has_results,
                           project_id=project_id,
                           count_jobs_in_queue_before_api_url_partial=url_for('count_jobs_in_queue'),
                           linker_api_url=url_for('schedule_job', job_name='linker', project_id=project_id),
                           download_api_url=url_for('download', 
                                project_type=project_type, project_id=project_id),
                           next_url=next_url)
  
    

@app.route('/web/view_results/<project_type>/<project_id>/', methods=['GET'])
def web_view_results(project_type, project_id):      
    if project_type == 'normalize':
        raise NotImplementedError
    
    proj = _init_project(project_type, project_id=project_id)
    
    res_file_name = 'm3_result.csv'

    # Identify rows to display
    proj.load_data('dedupe_linker', res_file_name)  

    certain_col_matches = proj.read_col_certain_matches()
    use_lower = True
    metrics = proj.infer('results_analysis', {'col_matches': certain_col_matches, 'lower':use_lower})

    # Choose the columns to display # TODO: Absolutely move this
    col_matches = proj.read_col_matches() # TODO: API this
    suffixes = ('_x', '_y')
    cols_to_display_match = []
    
    # TODO: fix for multiple selects of same column
    cols_to_display_match = []
    for col in [_col for match in col_matches for _col in match['source'] + match['ref']]: #\
                #+ proj.read_cols_to_return('ref'):
        if col in cols_to_display_match:
            cols_to_display_match.remove(col)
            cols_to_display_match.append(col + suffixes[0])
            cols_to_display_match.append(col + suffixes[1])
        else:
            cols_to_display_match.append(col)
            
    cols_to_display_match.append('__CONFIDENCE')
    print(cols_to_display_match)
        

    # Choose the columns to display for single source and single ref 
    # TODO: Absolutely change this
    cols_to_display_source = []
    for match in col_matches:
        for col in match['source']:
            if col in cols_to_display_source:
                cols_to_display_source.remove(col)
            else:
                cols_to_display_source.append(col)

    cols_to_display_ref = []
    for match in col_matches:
        for col in match['ref']:
            if col in cols_to_display_ref:
                cols_to_display_ref.remove(col)
            else:
                cols_to_display_ref.append(col)


    # Choose rows to display # TODO: Absolutely move this
    NUM_ROWS_TO_DISPLAY = 1000
    rows_to_display = list(proj.mem_data.index[proj.mem_data.__CONFIDENCE.notnull()])
    rows_to_display = rows_to_display[:NUM_ROWS_TO_DISPLAY + 1]
    
    # Generate display sample
    sample_params = {
                    'sample_ilocs': rows_to_display,
                    'drop_duplicates': True,
                    'cols_to_display': cols_to_display_match
                     }
    match_sample = proj.get_sample(None, None, sample_params)
    
    if certain_col_matches:
        sel = proj.mem_data.__CONFIDENCE.notnull()
        if use_lower:
            sel = sel & (proj.mem_data[certain_col_matches['source']].str.lower() \
                         != proj.mem_data[certain_col_matches['ref']].str.lower())
        else:
            sel = sel & (proj.mem_data[certain_col_matches['source']] \
                         != proj.mem_data[certain_col_matches['ref']])
        rows_to_display_error = list(proj.mem_data.index[sel])
        rows_to_display_error = rows_to_display_error[:NUM_ROWS_TO_DISPLAY + 1]
        
        sample_params = {
                        'sample_ilocs': rows_to_display_error,
                        'drop_duplicates': True
                         }        
        
        match_error_samples = proj.get_sample(None, None, sample_params)
    else:
        match_error_samples = []
    
    #    source_sample = proj.get_sample('INIT', proj.metadata['current']['source']['file_name'],
    #                                row_idxs=rows_to_display, columns=cols_to_display_source)
    #ref_sample = proj.get_sample('ref', 'INIT', proj.metadata['current']['ref']['file_name'],
    #                            row_idxs=rows_to_display, columns=cols_to_display_ref)

    return render_template('last_page_old.html', 
                           project_id=project_id,
                           
                           match_index=cols_to_display_match,
                           match_sample=match_sample,
                           match_error_samples=match_error_samples,                           
                           
                           source_index=cols_to_display_source,
                           ref_index=cols_to_display_ref,
                           # ref_sample=ref_sample,  
                           metrics=metrics,
                           download_api_url=url_for('download', 
                                project_type=project_type, project_id=project_id))

#==============================================================================
# API
#==============================================================================

# TODO: get_config if module_name is specified specific module, otherwise, entire project
#@app.route('/api/<project_type>/<project_id>/<module_name>/<file_name>/')
#def get_config(project_type, project_id, module_name=None, file_name=None):
#    '''See docs in abstract_project'''
#    proj = _init_project(project_type, project_id)
#    return proj.get_config(module_name, file_name)

#==============================================================================
# GENERIC API METHODS (NORMALIZE AND LINK)
#==============================================================================

@app.route('/api/new/<project_type>', methods=['POST'])
def new_project(project_type):
    '''
    Create a new project:
        
    GET:
        - project_type: "link" or "normalize"
        
    POST:
        - (description): project description
        - (display_name): name to show to user
        - (internal): (synonymous to public)
    
    '''
    _check_project_type(project_type)
    
    # TODO: include internal in form somewhere
    description = request.json.get('description', '')
    display_name = request.json.get('display_name', '')
    internal = request.json.get('internal', False)
    
    if internal and (not description):
        raise Exception('Internal projects should have a description')

    if project_type == 'normalize':
        proj = UserNormalizer(create_new=True, description=description, display_name=display_name)
    else:
        proj = UserLinker(create_new=True, description=description, display_name=display_name)

    return jsonify(error=False, 
                   project_id=proj.project_id)

@app.route('/api/delete/<project_type>/<project_id>', methods=['GET'])
def delete_project(project_type, project_id):
    """
    Delete an existing project (including all configuration, data and metadata)
    
    GET:
        - project_type: "link" or "normalize"
        - project_id
    """
    _check_project_type(project_type)
    # TODO: replace by _init_project
    if project_type == 'normalize':
        proj = UserNormalizer(project_id=project_id)
    else:
        proj = UserLinker(project_id=project_id)
    proj.delete_project()
    return jsonify(error=False)


@app.route('/api/metadata/<project_type>/<project_id>', methods=['GET'])
@cross_origin()
def metadata(project_type, project_id):
    '''
    Fetch metadata for project ID
    
    GET:
        - project_type: "link" or "normalize"
        - project_id
    '''
    proj = _init_project(project_type, project_id=project_id)
    resp = jsonify(error=False,
                   metadata=proj.metadata, 
                   project_id=proj.project_id)
    return resp


@app.route('/api/last_written/<project_type>/<project_id>', methods=['GET', 'POST'])
def get_last_written(project_type, project_id):
    """
    Get coordinates (module_name, file_name) of the last file written for a 
    given project.
    
    wrapper around: AbstractDataProject.get_last_written
    
    GET:
        - project_type: "link" or "normalize"
        - project_id
    POST:
        - module_name: if not null, get last file written in chosen module
        - file_name: if not null, get last file written with this given_name
        - before_module: (contains module_name) if not null, get coordinates 
                            for file written before the chosen module (with an 
                            order specified by MODULE_ORDER)
    """
    proj = _init_project(project_type, project_id)
    (module_name, file_name) = proj.get_last_written(request.json.get('module_name'), 
                          request.json.get('file_name'), 
                          request.json.get('before_module'))
    return jsonify(project_type=project_type, 
                   project_id=project_id, 
                   module_name=module_name, 
                   file_name=file_name)


@app.route('/api/download/<project_type>/<project_id>', methods=['GET', 'POST'])
@cross_origin()
def download(project_type, project_id):
    '''
    Download specific file from project.
    
    GET:
        - project_type: "link" or "normalize"
        - project_type
        
    POST:
        - module_name: Module from which to fetch the file
        - file_name
    
    '''
    project_id = secure_filename(project_id)

    proj = _init_project(project_type, project_id)
    data_params, _ = _parse_request()
    
    if data_params is None:
        data_params = {}
        
    file_role = data_params.get('file_role')
    module_name = data_params.get('module_name')
    file_name = data_params.get('file_name')

    if file_role is not None:
        file_role = secure_filename(file_role)
    if module_name is not None:
        module_name = secure_filename(module_name)
    if file_name is not None:
        file_name = secure_filename(file_name)

    (module_name, file_name) = proj.get_last_written(module_name, file_name)

    if module_name == 'INIT':
        return jsonify(error=True,
               message='No changes were made since upload. Download is not \
                       permitted. Please do not use this service for storage')

    file_path = proj.path_to(module_name, file_name)
    new_file_name = file_name.split('.csv')[0] + '_MMM.csv'
    return send_file(file_path, as_attachment=True, attachment_filename=new_file_name)


# TODO: get this from MODULES ?
API_SAMPLE_NAMES = ['standard', 'sample_mvs', 'sample_types']

@app.route('/api/sample/<project_type>/<project_id>', methods=['POST'])
@cross_origin()
def get_sample(project_type, project_id):
    '''
    Generate a sample.
    
    GET:
        - project_type
        - project_id
        
    POST:
        - data_params:
            - module_name
            - file_name
        - module_params:
            - sampler_module_name: (ex: 'sample_mvs'). 
                            TODO: explicit
            - module_params: (optional) parameters generated by the associated inference module 
                            (Usually the result of inference. ex: result of infer_mvs)
            - sample_params: (optional) parameters to use for sampling 
                            TODO: standardize and explicit
                            {
                            'restrict_to_selected': True or False (default True),
                            'num_rows': number of rows to return (default 50) (does not apply for non standard samplers)
                            'randomize': (default True) If false, will return first values
                            }
    '''
    print('Sample request is ', request.json)
    
    proj = _init_project(project_type=project_type, project_id=project_id)    
    data_params, all_params = _parse_request() # TODO: add size limit on params
    
    print('1', request.json)
    
    if all_params is None:
        all_params = dict()

    sampler_module_name = all_params.get('sampler_module_name', None)
    if sampler_module_name == 'standard':
        sampler_module_name = None
    
    module_params = all_params.get('module_params', {})
    sample_params = all_params.get('sample_params', {})
    sample_params.setdefault('restrict_to_selected', True)

    # Get sample
    proj.load_data(data_params['module_name'], 
                   data_params['file_name'], 
                   restrict_to_selected=sample_params['restrict_to_selected'])

    sample_params.setdefault('randomize', True)
    print('2', sample_params)
    sample_params.setdefault('num_rows', min(50, proj.mem_data.shape[0]))
    print('3', sample_params)

    if (sampler_module_name is not None) and (sampler_module_name not in API_SAMPLE_NAMES):
        raise ValueError('Requested sampler_module_name {0} is not valid. Valid'\
                         + 'modules are: {1}'.format(sampler_module_name, API_SAMPLE_NAMES))
    

    
    sample = proj.get_sample(sampler_module_name, module_params, sample_params)
    return jsonify(sample=sample)


@app.route('/api/exists/<project_type>/<project_id>', methods=['GET'])
@cross_origin()
def project_exists(project_type, project_id):
    '''
    Check if project exists
    
    GET:
        - project_type: "link" or "normalize"
        - project_id
    '''
    try:
        _init_project(project_type=project_type, project_id=project_id)
        return jsonify(exists=True)
    except Exception as exc: 
        return jsonify(exists=False)

@app.route('/api/download_config/<project_type>/<project_id>/', methods=['POST'])
@cross_origin()
def read_config(project_type, project_id):
    """
    Reads content of a config file
    
    GET:
        - project_type: "link" or "normalize"
        - project_id
        
    POST:
        - data: {
                "module_name": module to fetch from
                "file_name": file to fetch
                }    
    """
    # TODO: do not expose ?
    proj = _init_project(project_type=project_type, project_id=project_id)    
    data_params, _ = _parse_request() # TODO: add size limit on params
    
    file_name = data_params['file_name']
    
    # Check that the file_name is allowed:
    assert file_name in ['training.json', 'infered_config.json', 'config.json']
    
    result = proj.read_config_data(data_params['module_name'], file_name)
    return jsonify(result=result)



@app.route('/api/upload_config/<project_type>/<project_id>/', methods=['POST'])
@cross_origin()
def upload_config(project_type, project_id):
    """
    Writes the content of params
    
    GET:
        - project_type: "link" or "normalize"
        - project_id
        
    POST:
        - data_params: {
                "module_name": module to fetch from
                "file_name": file to fetch
                }
        - module_params: parameters to write
    
    """
    # TODO: do not expose ?
    proj = _init_project(project_type=project_type, project_id=project_id)    
    data_params, params = _parse_request() # TODO: add size limit on params
    
    file_name = data_params['file_name']
    
    # Check that the file_name is allowed:
    assert file_name in ['training.json', 'config.json']
    
    proj.upload_config_data(params, data_params['module_name'], file_name)
    return jsonify(error=False)

#==============================================================================
# NORMALIZE API METHODS (see also SCHEDULER)
#==============================================================================

@app.route('/api/normalize/select_columns/<project_id>', methods=['POST'])
def add_selected_columns(project_id):
    """
    Select columns to modify in normalization project. 
    
    /!\ If column selection includes new columns 
    
    GET:
        - project_id

    POST: 
        - columns: [list of columns]
        
    """
    selected_columns = request.json['columns']
    proj = UserNormalizer(project_id=project_id)
    proj.add_selected_columns(selected_columns)    
    return jsonify(error=False)

@app.route('/api/normalize/upload/<project_id>', methods=['POST'])
@cross_origin()
def upload(project_id):
    '''
    Uploads files to a normalization project. (NB: cannot upload directly to 
    a link type project). 
    
    Also creates the mini version of the project
    
    GET:
        - project_id: ID of the normalization project
        
    POST:
        
      file: (csv file) A csv to upload to the chosen normalization project
                  NB: the "filename" property will be used to name the file
      json:
        - module_params:
            - make_mini: (default True) Set to False to NOT create a mini version of the file
            - sample_size
            - randomize
    '''
    # Load project
    proj = UserNormalizer(project_id=project_id) 
    _, module_params = _parse_request()   
    if module_params is None:
        module_params = {}
    make_mini = module_params.get('make_mini', True)
    
    # Upload data
    file = request.files['file']
    if file:
        proj.upload_init_data(file.stream, file.filename)
    else:
        raise Exception('Empty file')
        
    # Make mini
    if make_mini:
        proj.make_mini(module_params)
        
        # Write transformations and log
        proj.write_data()    
        proj.write_log_buffer(True)
    
        
    run_info = proj.read_config_data('INIT', 'run_info.json')
    return jsonify(run_info=run_info, project_id=proj.project_id)


@app.route('/api/normalize/make_mini/<project_id>', methods=['POST'])
@cross_origin()
def make_mini(project_id):
    '''
    Create sample version of selected file (call just after upload).
    
    GET:
        - project_id
    POST:
        - data_params: 
                        {
                        module_name: 'INIT' (mandatory to be init)
                        file_name: 
                        }
        - module_params: {
                            sample_size: 
                            randomize:
                        }
    '''
    data_params, module_params = _parse_request()   
    proj = UserNormalizer(project_id=project_id)
    
    proj.load_data(data_params['module_name'], data_params['file_name'])
    proj.make_mini(module_params)
    
    # Write transformations and log
    proj.write_data()    
    proj.write_log_buffer(True)

#==============================================================================
# LINK API METHODS (see also SCHEDULER)
#==============================================================================

@app.route('/api/link/select_file/<project_id>', methods=['POST'])
def select_file(project_id):
    '''    
    TODO: FIX THIS MESS  !!!!
    
    Choose a file to use as source or referential for merging
    send {file_role: "source", project_id: "ABCYOUANDME", internal: False}
    
    GET:
        - project_id: ID for the "link" project
        
    POST:
        - file_role: "ref" or "source". Role of the normalized file for linking
        - project_id: ID of the "normalize" project to use for linking
    '''
    proj = UserLinker(project_id)
    params = request.json
    proj.add_selected_project(file_role=params['file_role'], 
                           internal=params.get('internal', False), # TODO: remove internal
                           project_id=params['project_id'])
    return jsonify(error=False)


@app.route('/api/link/add_column_matches/<project_id>/', methods=['POST'])
@cross_origin()
def add_column_matches(project_id):
    """
    Add pairs of columns to compare for linking.
    
    wrapper around UserLinker.add_col_matches
    
    GET: 
        - project_id: ID for the "link" project
        
    POST:
        - column_matches: [list object] column matches (see doc in original function)
    """
    column_matches = request.json['column_matches']
    proj = UserLinker(project_id=project_id)
    proj.add_col_matches(column_matches)
    return jsonify(error=False)
    

@app.route('/api/link/add_column_certain_matches/<project_id>/', methods=['POST'])
@cross_origin()
def add_column_certain_matches(project_id):
    '''
    Specify certain column matches (exact match on a subset of columns equivalent 
    to entity identity). This is used to test performances.
    
    wrapper around UserLinker.add_col_certain_matches
    
    GET:
        - project_id: ID for "link" project
        
    POST:
        - column_certain_matches: {dict object}: (see doc in original function)
    
    '''
    column_matches = request.json['column_certain_matches']
    proj = UserLinker(project_id=project_id)
    proj.add_col_certain_matches(column_matches)
    return jsonify(error=False)



@app.route('/api/link/add_columns_to_return/<project_id>/<file_role>/', methods=['POST'])
@cross_origin()
def add_columns_to_return(project_id, file_role):
    '''
    Specify columns to be included in download version of file. For link project 
    
    # TODO: shouldn't this be for normalize also ?
    
    wrapper around UserLinker.add_cols_to_return
    
    GET:
        project_id: ID for "link" project
        file_role: "ref" or "source"
    '''
    columns_to_return = request.json
    proj = UserLinker(project_id=project_id)
    proj.add_cols_to_return(file_role, columns_to_return)    
    return jsonify(error=False)

#==============================================================================
# SCHEDULER
#==============================================================================

# TODO: job_id does not allow to call all steps of a pipeline at once
# TODO: put all job schedulers in single api (assert to show possible methods) or use @job   

# TODO: get this from MODULES ?
API_MODULE_NAMES = ['infer_mvs', 'replace_mvs', 'infer_types', 'recode_types', 
                    'concat_with_init', 'run_all_transforms', 'create_labeller', 'linker']

@app.route('/api/schedule/<job_name>/<project_id>/', methods=['GET', 'POST'])
@cross_origin()
def schedule_job(job_name, project_id):    
    '''
    Schedule module runs
    
    GET:
        - job_name: name of module to run (full list in API_MODULE_NAMES)
        - project_id
    POST:
        - data_params: the data to transform (see specific module docs)
        - module_params: how to transform the data (see spectific module docs)
    
    ex: '/api/schedule/infer_mvs/<project_id>/'
    '''
    assert job_name in API_MODULE_NAMES
    data_params, module_params = _parse_request()
    
    job_id = project_id + '_' + job_name
    
    
    #    job_name = 'test_long'
    #    import time
    #    
    #    for x in range(3): job = q.enqueue_call(func='api_queued_modules._' + job_name, args=(project_id, data_params, module_params), result_ttl=5000, job_id='test')
    #        
    #    time.sleep(1)
    #    job.cancel(); print(q.jobs)
    #    
    #    import pdb; pdb.set_trace()
    
    #TODO: remove and de-comment unfer
    job = q.enqueue_call(
            func='api_queued_modules._' + job_name,
            args=(project_id, data_params, module_params), 
            result_ttl=5000, 
            job_id=job_id, 
            #depends_on=project_id
    )        
    
    # 
    job_id = job.get_id()
    print(job_id)
    return jsonify(job_id=job_id,
                   job_result_api_url=url_for('get_job_result', job_id=job_id))    
    

@app.route('/queue/result/<job_id>', methods=['GET'])
def get_job_result(job_id):
    '''
    Fetch the json output of a module run scheduled by schedule_job. Will return 
    a 202 code if job is not yet complete and 404 if job could not be found.
    
    GET:
        - job_id: as returned by schedule_job
    '''
    try:
        job = Job.fetch(job_id, connection=conn)
    except:
        return jsonify(error=True, message='job_id could not be found', completed=False), 404
    
    if job.is_finished:
        #return str(job.result), 200
        return jsonify(completed=True, result=job.result)
    
    else:
        return jsonify(completed=False), 202

@app.route('/queue/cancel/<job_id>', methods=['GET'])
def cancel_job(job_id):
    '''
    Remove job from queue
    
    # TODO: make this work
    
    GET:
        - job_id: as returned by schedule_job
    '''
    
    try:
        job = Job.fetch(job_id, connection=conn)
        job.cancel()
        return jsonify(job_canceled=True, completed=False)
    except:
        return jsonify(job_canceled=False,
                       error=True, 
                       message='job_id could not be found', 
                       completed=False), 404
    



@app.route('/queue/num_jobs/<job_id>', methods=['GET'])
def count_jobs_in_queue_before(job_id):
    '''
    Returns the number of jobs preceding job_id or -1 if job is no longer in queue.
    
    GET:
        - job_id: as returned by schedule_job
    
    '''
    job_ids = q.job_ids
    if job_id in job_ids:
        return jsonify(num_jobs=job_ids.index(job_id))
    else:
        return jsonify(num_jobs=-1)
    # TODO: check if better to return error
    
@app.route('/queue/num_jobs/', methods=['GET'])
def count_jobs_in_queue():
    '''Returns the number of jobs enqueued'''
    # TODO: change for position in queue
    num_jobs = len(q.job_ids)
    return jsonify(num_jobs=num_jobs)

#==============================================================================
    # Admin
#==============================================================================

@app.route('/api/projects/<project_type>', methods=['GET'])
def list_projects(project_type):
    '''
    TODO: TEMPORARY !! DELETE FOR PROD !!!
    '''
    admin = Admin()
    list_of_projects = admin.list_project_ids(project_type)
    return jsonify(list_of_projects)



if __name__ == '__main__':
    socketio.run(app, host='0.0.0.0', port=5000, debug=True)
